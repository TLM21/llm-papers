# llm-papers
  Best LLM papers

# Format 
  - [ Month/Year ] [ **Title **] [ PDF ] [ GIT ] [ Takeaway ] 

# Papers
  - [ 02/2024 ] [ **More Agents Is All You Need**] [ [ PDF ](https://arxiv.org/pdf/2402.05120v1) ] [ [ GIT ](https://anonymous.4open.science/r/more_agent_is_all_you_need/README.md) ] [ In this study, increasing instantiated LLM agents improves performance on complex tasks using a straightforward sampling-and-voting method. Performance gains vary with task difficulty, specifically inherent difficulty (I), reasoning step lengths (S), and prior probability (K). ] 

  - [ 05/2023 ] [ **Improving Factuality and Reasoning in Language Models through Multiagent Debate** ] [ [ PDF ](https://arxiv.org/pdf/2305.14325) ] [ [ GIT ](https://composable-models.github.io/llm_debate/) ] [ This study found that increasing the number of agents and debate rounds improves accuracy, though these factors were limited by cost constraints. Extrapolation suggests that while more agents and rounds provide marginal improvements, these gains diminish over time. Additionally, longer debate prompts yield better performance as the number of rounds increases. Summarization can also serve as a tool to extend the context, further enhancing the models' capabilities. ] 
 
  - [ 03/2023 ] [ **Tree of Thoughts: Deliberate Problem Solving with Large Language Models** ] [ [ PDF ](https://arxiv.org/pdf/2305.10601) ] [ [ GIT ](https://github.com/princeton-nlp/tree-of-thought-llm) ] [ The Tree of Thoughts (ToT) framework enhances language models by integrating classical human-like problem-solving methods, resulting in higher-quality answers at the cost of increased computational power. ] 

  - [ 06/2017 ] [ **Attention Is All You Need** ] [ [ PDF ](https://arxiv.org/pdf/1706.03762) ] [ [ GIT ](https://github.com/tensorflow/tensor2tensor) ] [ Authors introduce, Transformer model as the first sequence transduction system entirely reliant on attention mechanisms, eliminating the need for recurrent layers commonly found in encoder-decoder structures. ] 
