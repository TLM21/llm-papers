# llm-papers
  Best LLM papers

# Format 
  - [ Month/Year ] [ **Title **] [ PDF ] [ GIT ] [ Takeaway ] 

# Papers
  - [ 02/2024 ] [ **More Agents Is All You Need**] [ [ PDF ](https://arxiv.org/pdf/2402.05120v1) ] [ [ GIT ](https://anonymous.4open.science/r/more_agent_is_all_you_need/README.md) ] [ In this study, increasing instantiated LLM agents improves performance on complex tasks using a straightforward sampling-and-voting method. Performance gains vary with task difficulty, specifically inherent difficulty (I), reasoning step lengths (S), and prior probability (K). ] 

 - [ 08/2023 ] [ **AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors** ] [ [ PDF ](https://arxiv.org/pdf/2308.10848) ] [ [ GIT ](https://github.com/OpenBMB/AgentVerse/) ] [ This study introduces AGENTVERSE, a multi-agent framework that enhances task performance through collaboration among autonomous agents using LLMs. The framework consists of four pivotal stages: (1) Expert Recruitment: Adjusting the agent group composition based on task progression. (2) Collaborative Decision-Making: Engaging agents in joint strategy discussions. (3) Action Execution: Implementing actions through environmental interaction. (4) Evaluation: Assessing outcomes and providing feedback for further refinement. AGENTVERSE outperforms single agents in tasks such as text understanding, reasoning, coding, tool utilization, and embodied AI, with emergent collaborative behaviors enhancing group efficiency. ] 

  - [ 05/2023 ] [ **Improving Factuality and Reasoning in Language Models through Multiagent Debate** ] [ [ PDF ](https://arxiv.org/pdf/2305.14325) ] [ [ GIT ](https://composable-models.github.io/llm_debate/) ] [ This study found that increasing the number of agents and debate rounds improves accuracy, though these factors were limited by cost constraints. Extrapolation suggests that while more agents and rounds provide marginal improvements, these gains diminish over time. Additionally, longer debate prompts yield better performance as the number of rounds increases. Summarization can also serve as a tool to extend the context, further enhancing the models' capabilities. ] 
 
  - [ 03/2023 ] [ **Tree of Thoughts: Deliberate Problem Solving with Large Language Models** ] [ [ PDF ](https://arxiv.org/pdf/2305.10601) ] [ [ GIT ](https://github.com/princeton-nlp/tree-of-thought-llm) ] [ The Tree of Thoughts (ToT) framework enhances language models by integrating classical human-like problem-solving methods, resulting in higher-quality answers at the cost of increased computational power. ] 

  - [ 06/2017 ] [ **Attention Is All You Need** ] [ [ PDF ](https://arxiv.org/pdf/1706.03762) ] [ [ GIT ](https://github.com/tensorflow/tensor2tensor) ] [ Authors introduce, Transformer model as the first sequence transduction system entirely reliant on attention mechanisms, eliminating the need for recurrent layers commonly found in encoder-decoder structures. ] 
